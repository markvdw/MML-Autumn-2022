\paragraph{Question \ref{q:MLE-Niid}} 
a) 
    Our model assumes Gaussian density function defined over $N$ iid data samples collected in $x \in \Reals^N$ as $x=[x_1, \dots, x_N]\transpose$.
    Given the iid assumption, we can find the joint distribution by multiplying the same distribution $N$ times:
    $$p(x \mid \mu, \sigma) = \prod_{i=1}^N \mathcal{N}(x_i; \mu, \sigma^2)=\prod_{i=1}^N \frac{1}{\sigma \sqrt{2\pi}}\exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)$$

b) 
    The log-likelihood of the above distribution can be defined as:
    \begin{align*}
    \mathcal{L}(\mu, \sigma) &= \log p(x|\mu,\sigma^2) = \log \prod_{i=1}^N \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right) \\
    &= -N \log (\sqrt{2\pi \sigma^2}) - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2}
    \end{align*}
    Maximum likilehood estimates for $\mu, \sigma$ are obtained by maximizing the above likelihood function, which corresponds to:
    $$\mu^*, \sigma^{2*} = \argmax_{\mu, \sigma^2} -N \log (\sqrt{2\pi \sigma^2}) - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2}$$
    To find a maximum, we need to ensure that the derivatives w.r.t.~$\mu$ and $\sigma^2$ are \emph{simultaneously} zero. To do this, we should start by finding both partial derivatives. We start with $\mu$:
    \begin{gather*}
    \pderiv[\mathcal L]{\mu} = \frac{\partial (-N \log (\sigma \sqrt{2\pi}) - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2})}{\partial \mu} = 0 \\
    2\sum_{i=1}^N \frac{(x_i - \mu)}{2\sigma^2} = 0 \\
    \mu^* = \frac{1}{N} \sum_{i=1}^N x_i
    \end{gather*}
    We find that this can be set to zero \emph{independently} of $\sigma^2$.

    For variance estimation we differentiate the likelihood function w.r.t $\sigma^2$ at the obtained $\mu^*$ and set that to zero:
    \begin{gather}
    \frac{\partial (-N \log (\sqrt{2\pi\sigma^2}) - \sum_{i=1}^N \frac{(x_i - \mu^*)^2}{2\sigma'^2})}{\partial \sigma^2} = 0 \\
    -\frac{N}{2 \sigma^2} + \sum_{i=1}^N \frac{(x_i - \mu^*)^2}{2\sigma^4} = 0 \\
    \sigma^{2*} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu^*)^2
    \end{gather}

    Finally, we compute the Hessian to verify that our solution is a maximum:
    $$H = \begin{bmatrix}
            \frac{\partial^2 (-N \log (\sigma \sqrt{2\pi}) - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma'^2})}{\partial \mu^2} & 
            \frac{\partial^2 (-N \log (\sigma \sqrt{2\pi}) - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2})}{\partial (\sigma^2) \partial \mu} \\ 
            \frac{\partial^2 (-N \log (\sigma \sqrt{2\pi}) - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma'^2})}{\partial \mu \partial (\sigma^2)} & 
            \frac{\partial^2 (-N \log (\sigma \sqrt{2\pi}) - \sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2})}{\partial (\sigma^2)^2} 
        \end{bmatrix} = 
        \begin{bmatrix} 
            -\frac{N}{\sigma^2} &  
            - \sum_{i=1}^N \frac{(x_i - \mu)}{\sigma^4} \\ 
            - \sum_{i=1}^N \frac{(x_i - \mu) }{\sigma^4} & 
            \frac{N}{2 \sigma^4} - \sum_{i=1}^N \frac{(x_i - \mu)^2}{\sigma^6} 
        \end{bmatrix}$$
    
    Determinant of the computed Hessian at $\mu^*, \sigma^{2*}$:
\begin{align}
\det(H) &= -\frac{N^2}{2 (\sigma^{2*})^3} + \frac{N}{(\sigma^{2*})^4} \sum_{i=1}^N (x_i - \mu^*)^2 - \frac{1}{(\sigma^{2*})^4} (\sum_{i=1}^N(x_i - \mu^*))^2 \\
&= -\frac{N^2}{2 (\sigma^{2*})^3} + \frac{N^2}{(\sigma^{2*})^4} \sum_{i=1}^N \frac{(x_i - \mu^{*})^2}{N} - \frac{1}{(\sigma^{2*})^4} (\sum_{i=1}^N(x_i - \mu^{*}))^2 \\
&= \frac{N^2}{2 (\sigma^{2*})^3}  - \frac{N}{(\sigma^{2*})^3} \\
&= \frac{N^2 -2N}{2 (\sigma^{2*})^3}
\end{align}

For $N \geq 2$, we have that $\det(H) \geq 0$. Keeping in mind that $\det(H) = \prod_n \lambda_n$, where $\lambda_n$ are the eigenvalues of $H$, and that $H[0,0]$ is negative, we conclude the $\mu^*$ and $\sigma^{2*}$ results in maximizing logliklihood of the distribution.

    
    

\paragraph{Question \ref{q:MLEReg}}

Let $\mat X$ be data matrix of all inputs with $\vy$ as the outputs and $f(\vx; \vw, \beta) = \vw^T\vx + \beta = \theta^T[\vx, 1]$ be a linear regression model, where $\theta=[\vw, \beta]$ is a vector of parameters including bias.

Let's consider two cases for estimating parameters $\theta$

\textbf{case 1:} Maximum Likelihood Estimate (MLE): 
$\theta^* = \argmax_{\theta} \mathcal{L}(y; f(\vx; \theta), \mathbb{I}\sigma^2)$

\textbf{case 2:} Minimum Squared Estimate (MSE):
$\theta^* = \argmin_{\theta} \frac{1}{N}\sum_{i=1}^N(y_i - f(\vx_i; \theta))^2$

Task is to show that both estimates are same.



Consider case 1: $\theta^* = \argmax_{\theta} \mathcal{L}(y; f(\vx; \theta), \mathbb{I}\sigma^2)$

$$ \Rightarrow \frac{\partial \mathcal{L}(y; f(\vx; \theta), \mathbb{I}\sigma^2)}{\partial \theta)} = 0 $$

% $$ \Rightarrow \frac{\partial \log \prod_{i=1}^N \sigma^{-1}(2\pi)^{-1/2} \exp(-0.5\sigma^{-2}(y_i - \theta^T x)^2}{\partial \theta} = 0 $$

$$ \Rightarrow \frac{-0.5\sigma^{-2}\partial \sum_{i=1}^N (y_i - \theta^T\vx_i)^2}{\partial \theta} = 0 $$

$$ \Rightarrow \frac{\partial \frac{1}{N}\sum_{i=1}^N (y_i - \theta^T\vx_i)^2}{\partial \theta} = 0 $$

As the above expression is linear in $\theta$, it can be also be seen as: $\theta^* = \argmin_{\theta} \frac{1}{N}\sum_{i=1}^N(y_i - f(\vx_i; \theta))^2$, resulting in MSE estimates.




\paragraph{Question \ref{q:chainrule}}
\begin{enumerate}[label=\alph*.]
\item $f(x) = \log (x^4) \sin (x^3)$
        $$\frac{\calcd f(x)}{\calcd x} = \sin(x^3) \frac{\calcd \log(x^4)}{\calcd x} + \log (x^4)\frac{\calcd \sin(x^3)}{\calcd x}$$
        $$\Rightarrow f'(x) = \sin(x^3)\frac{4}{x} + 12\log(x)\cos(x^3)x^2$$
        
\item $f(x) = (1 + \exp(-x))^{-1}$
         $$\frac{\calcd f(x)}{\calcd x} = \frac{\calcd (1 + \exp(-x))^{-1}}{\calcd x}$$
         $$\Rightarrow f'(x) = \frac{\calcd (1 + \exp(-x))^{-1})}{\calcd(1 + \exp(-x))} \frac{\calcd(1 + \exp(-x))}{\calcd x}$$
         $$\Rightarrow f'(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2}$$
        
\item $f(x) = \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$
         $$\frac{\calcd f(x)}{\calcd x} = \frac{\calcd \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)}{\calcd x}$$
         $$\Rightarrow f'(x) = -\frac{(x-\mu)}{\sigma^2}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

\end{enumerate}
