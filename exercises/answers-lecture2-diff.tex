\section{Answers Lecture 2: Vector Differentiation}
\paragraph{\questionref{q:circle}} Answer discussed in lectures.

\paragraph{\questionref{q:ind-not-proof}}

\begin{enumerate}
\item $\textbf{x}^TA\textbf{y} = \textbf{y}^TA\textbf{x}$ if $A=A^T$.

Note that $A=A^T \implies a_{ij}=a_{ij}$. We first write the product in terms of two vectors, $\textbf{x}$ and $A\textbf{y}$, then rearrange the terms considering $A\textbf{y}$ as a vector with components $(A\textbf{y})_i = \sum_{j=1}^N a_{ij}y_j$, and use $a_{ij} = a_{ji}$:
\begin{align*}
\textbf{x}^TA\textbf{y} = \sum_{i=1}^N x_i(A\textbf{y})_i = \sum_{i=1}^N x_i \sum_{j=1}^N a_{ij}y_j = \sum_{j=1}^N \sum_{i=1}^Ny_j a_{ji} x_i = \sum_{j=1}^N y_j (A\textbf{x})_j = \textbf{y}^TA\textbf{x}.
\end{align*}

\item $\textbf{x}^T\textbf{y} = Tr(\textbf{x}^T\textbf{y}) = Tr(\textbf{y}^T\textbf{x}), \textbf{x},\textbf{y}\in\mathbb{R}^D$.

Considering $\textbf{x}^T\textbf{y}\in\mathbb{R}$, we have $\textbf{x}^T\textbf{y}=Tr(\textbf{x}^T\textbf{y})$. Then, we need to check whether $\textbf{x}^T\textbf{y} = \textbf{y}^T\textbf{x}$, which we can show by rearranging the terms of the vector product
\begin{align*}
\textbf{x}^T\textbf{y} = \sum_{i=1}^N x_i y_i = \sum_{i=1}^N y_i x_i = \textbf{y}^T\textbf{x}.
\end{align*}

\item $Tr(ABC) = Tr(CAB)$. We assume $A\in\mathbb{R}^{D\times E}, B\in\mathbb{R}^{E\times F}$, and $C\in\mathbb{R}^{F\times D}.$

We start by inspecting the terms involving $Tr(ABC)$,
\begin{align*}
Tr(ABC) = \sum_{i=1}^D \big( (AB) C\big)_{ii} = \sum_{i=1}^D \sum_{j=1}^F (AB)_{ij} c_{ji} = \sum_{i=1}^D \sum_{j=1}^F \sum_{k=1}^E a_{ik} b_{kj} c_{ji}.
\end{align*}
Just by swapping the summations we can get the following identity: $Tr(ABC) = Tr(CAB) = Tr(BCA)$. We show $Tr(ABC) = Tr(CAB)$ as an example:
\begin{align*}
Tr(ABC) = \sum_{i=1}^D \sum_{j=1}^F \sum_{k=1}^E a_{ik} b_{kj} c_{ji} = \sum_{j=1}^F \sum_{k=1}^E \sum_{i=1}^D c_{ji}a_{ik}b_{kj} = \sum_{j=1}^F \sum_{k=1}^E (CA)_{jk}b_{kj} = \sum_{j=1}^F (CAB)_{jj} = Tr(CAB).
\end{align*}

\end{enumerate}

\paragraph{\questionref{q:mml55-56}}

\begin{enumerate}[label=\alph*]
    \item $f(\textbf{x}) = \sin(x_1)\cos(x_2), \quad \textbf{x}\in \mathbb{R}^2$
\[
\frac{\partial f}{\partial\textbf{x}} \in \mathbb{R}^{1\times 2}
\]
\begin{align*}
\frac{\partial f}{\partial\textbf{x}} &= \bigg[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2} \bigg]\\
&=\bigg[\cos(x_1)\cos(x_2), -\sin(x_1)\sin(x_2) \bigg]
\end{align*}
\item $f(\textbf{x}) = \textbf{x}^T\textbf{y}, \quad \textbf{x},\textbf{y}\in \mathbb{R}^n$
\[
\frac{\partial f}{\partial\textbf{x}} \in \mathbb{R}^{1\times n}
\]
We can solve this directly using basic rules of vector calculus
\[
\frac{\partial f}{\partial\textbf{x}} = \frac{\partial (\textbf{x}^T\textbf{y})}{\partial \textbf{x}} = \textbf{y}^T
\]
We can confirm this result holds with index notation. First, let us calculate the value $f(\textbf{x})$
\[
f(\textbf{x}) = \textbf{x}^T\textbf{y} = \sum_{i=1}^n x_iy_i
\]
\[
\frac{\partial f}{\partial x_j} = \frac{\partial}{\partial x_j}\sum_{i=1}^n x_iy_i =  \sum_{i=1}^n \frac{\partial x_j}{\partial x_i}y_i = \sum_{i=1}^n \delta_{ij}y_i = y_j
\]
\[
\frac{\partial f}{\partial\textbf{x}} = \left[\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}\right] = \left[y_1, \dots, y_n\right] = \textbf{y}^T
\]
\item $\textbf{f}(x) = \textbf{x}\textbf{x}^T, \quad \textbf{x}\in \mathbb{R}^n$
\[
\frac{\partial \textbf{f}}{\partial\textbf{x}} \in \mathbb{R}^{(n\times n)\times n}
\]
\[
\frac{\partial \textbf{f}}{\partial\textbf{x}}= C\quad \text{where $C$ is a 3D tensor.}
\]
\[
c_{ijk} = \frac{\partial f(\textbf{x})_{ij}}{\partial x_k}
\]
\[
\textbf{x}\textbf{x}^T = \begin{pmatrix}
    x_1   \\
    \vdots \\
    x_n \\
\end{pmatrix}
\begin{pmatrix}
    x_1 & \dots & x_n \\
\end{pmatrix} = \begin{pmatrix}
    x^2_1 & x_1 x_2 & \dots & x_1 x_n \\
    x_2 x_1 & x^2_2 &    & \vdots \\
    \vdots & & \ddots &  \vdots \\
    x_n x_1 & \dots & \dots & x^2_n \\
\end{pmatrix}
\]
\[
c_{ijk} = \frac{\partial (x_i x_j)}{\partial x_k} = \frac{\partial x_i}{\partial x_k} x_j + \frac{\partial x_j}{\partial x_k} x_i = \delta_{ik}x_j + \delta_{jk}x_i=  \begin{cases}
0 \quad &\text{if }k\neq i \text{ and } k\neq j\\
x_i \quad &\text{if }k=j \text{ and } i\neq j\\
x_j \quad &\text{if }k=i \text{ and } i\neq j\\
2x_i \quad &\text{if }k=i=j\\
\end{cases}
\]

    \item  $f(\textbf{t}) = \sin\big(\log(\textbf{t}^T\textbf{t})\big) \quad \textbf{t} \in \mathbb{R}^D$
    
We directly apply the chain rule
\[
\frac{\partial f}{\partial \textbf{t}} = \frac{\partial \sin\big(\log(\textbf{t}^T\textbf{t})\big)}{\partial \log(\textbf{t}^T\textbf{t})} \cdot \frac{\partial \log(\textbf{t}^T\textbf{t})}{\partial (\textbf{t}^T\textbf{t})} \cdot \frac{\partial (\textbf{t}^T\textbf{t})}{\partial \textbf{t}} 
\]
All of the terms are one dimensional except for $\frac{\partial (\textbf{t}^T\textbf{t})}{\partial \textbf{t}} \in\mathbb{R}^{1\times D}$. Let us calculate the value using the notation for vector calculus in the lectures. As in 5.5, we first calculate the value of $\textbf{t}^T\textbf{t}$ and its derivative w.r.t. $t_i$.
\[
\textbf{t}^T\textbf{t} = \sum_{i=1}^D t_i^2, \quad \frac{\partial (\textbf{t}^T\textbf{t})}{\partial t_i} = 2t_i
\]
\[
\frac{\partial (\textbf{t}^T\textbf{t})}{\partial \textbf{t}} = \left[\frac{\partial (\textbf{t}^T\textbf{t})}{\partial t_1}, \dots, \frac{\partial (\textbf{t}^T\textbf{t})}{\partial t_D}\right] = \left[2t_1 \dots, 2t_D\right] = 2\textbf{t}^T
\]

We can now use this result to proceed with the derivative of $f(\textbf{t})$.
\[
\frac{\partial f}{\partial \textbf{t}} = \cos \big(\log(\textbf{t}^T\textbf{t})\big) \cdot \frac{1}{\textbf{t}^T\textbf{t}} \cdot 2\textbf{t}^T
\]
\[
\frac{\partial f}{\partial \textbf{t}} = 2\textbf{t}^T \frac{\cos\big(\log(\textbf{t}^T\textbf{t})\big)}{\textbf{t}^T\textbf{t}}
\]

\item $f(X) = tr(AXB), \quad A\in \mathbb{R}^{D\times E},
X\in \mathbb{R}^{E\times F},
B\in \mathbb{R}^{F\times D}$

Use index notation:
\[
f(X) = tr(AXB) = \sum_{i=1}^D (AXB)_{ii}
\]
In order to fully compute $f(X)$, we need to calculate $(AXB)_{ii}$
\[
(AXB)_{ii} = \sum_{k=1}^F (AX)_{ik}b_{ki} = \sum_{k=1}^F \left(\sum_{l=1}^E a_{il}x_{lk}\right)b_{ki}
\]
Thus
\[
f(X) = \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}x_{lk}b_{ki}
\]
Now we can just calculate the derivative using index notation
\[
\frac{\partial f}{\partial x_{nm}} = \frac{\partial}{\partial x_{nm}} \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}x_{lk}b_{ki} = \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}\frac{\partial x_{lk}}{\partial x_{nm}}b_{ki} = \sum_{i=1}^D\sum_{k=1}^F \sum_{l=1}^E a_{il}\delta_{ln}\delta_{km}b_{ki}
\]
Notice that in the last expression, all the terms in the summation cancel except when $k=m$ and $l=n$. Therefore
\[
\frac{\partial f}{\partial x_{nm}} = \sum_{i=1}^D a_{in}b_{mi} = \sum_{i=1}^D b_{mi}a_{in} = (BA)_{mn}
\]
Using this last result, we can calculate the derivative w.r.t. X.
\[
\frac{\partial f}{\partial X} = (BA)^T = A^TB^T
\]

Alternative proof: Use properties 4.19 and 5.100 from the MML book.
From 4.19
\[
f(X) = tr(AXB) = tr(XBA) = tr(XC), \quad C = BA
\]
and from 5.100
\[
\frac{\partial f}{\partial X} = \frac{\partial tr(XC)}{\partial X} = tr\left(\frac{\partial (XC)}{\partial X}\right), \quad \text{where } \frac{\partial (XC)}{\partial X}\in \mathbb{R}^{(E \times E) \times (E \times F)}
\]

We need to calculate $\frac{\partial (XC)_{ij}}{\partial x_{kl}}$, and we find convenient to write the pairs $i,j$ of the product $IXC$, where $I \in \mathbb{R}^{E\times E}$ is the identity matrix.
\[
(IXC)_{ij} = \sum_{e=1}^E \sum_{f=1}^F \delta_{ie}x_{ef}c_{fj}
\]
\[
\frac{\partial (XC)_{ij}}{\partial x_{kl}} = \frac{\partial (IXC)_{ij}}{\partial x_{kl}} = \delta_{ik}c_{lj}
\]
in the prevous expression, all the terms in the sum vanish except the ones that contain $x_{kl}$ in it.

Now, we take into account the definition of the trace for any 4D tensor $T\in\mathbb{R}^{(N\times N)\times (P \times Q)}$ given in the MML book:
\[
tr(T)_{ij} = \sum_{k=1}^N a_{kkij}, \quad \text{where } tr(T) \in \mathbb{R}^{P\times Q}
\]

We use this definition to calculate our result.
\[
tr\left(\frac{\partial (XC)}{\partial X}\right)_{ij} = \sum_{k=1}^{E} \frac{\partial (XC)_{kk}}{\partial x_{ij}} = \sum_{k=1}^{E} \delta_{ki} c_{jk} = c_{ji}
\]
all the terms will be 0 except when $k=i$.
\[
tr\left(\frac{\partial (XC)}{\partial X}\right) = C^T = (BA)^T = A^TB^T
\]
\end{enumerate}


\paragraph{\questionref{q:chain-rule}}

\begin{enumerate}[label=\alph*.]
    \item $f(z) = \log(1 + z), \quad z = \textbf{x}^T\textbf{x}, \quad \textbf{x}\in\mathbb{R}^D$
    
\[
\frac{\partial f}{\partial \textbf{x}} = \frac{\partial f}{\partial z} \frac{\partial z}{\partial \textbf{x}} = \frac{\partial \log(1 + z)}{\partial z} \frac{\partial (\textbf{x}^T\textbf{x})}{\partial \textbf{x}} = \frac{2\textbf{x}^T}{1 + z} = \frac{2\textbf{x}^T}{1 + \textbf{x}^T\textbf{x}} 
\]
Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^D, \quad \frac{\partial f}{\partial z} \in \mathbb{R}, \quad \frac{\partial z}{\partial \textbf{x}} \in \mathbb{R}^D
\]
    
    \item $f(\textbf{z}) = \sin(\textbf{z}), \quad \textbf{z} = A\textbf{x} + \textbf{b}, \quad A\in\mathbb{R}^{E\times D}, \textbf{x}\in\mathbb{R}^D, \textbf{b}\in\mathbb{R}^E$

\[
\frac{\partial f}{\partial \textbf{x}} = \frac{\partial f}{\partial \textbf{z}} \frac{\partial \textbf{z}}{\partial \textbf{x}} = \frac{\partial \sin(\textbf{z})}{\partial \textbf{z}} \frac{\partial (A\textbf{x} + \textbf{b})}{\partial \textbf{x}}
\]
Notice that $\frac{\partial f}{\partial \textbf{z}} \in \mathbb{R}^{E\times E}$. We already know that $\sin(\cdot)$ is applied to each element independently, thus
\[
\frac{\partial f_i}{\partial z_j}=\begin{cases}
0 \quad &\text{if }i\neq j\\
\cos(z_i) \quad &\text{if }i=j\\
\end{cases}
\]
We also have $\frac{\partial \textbf{z}}{\partial \textbf{x}} \in \mathbb{R}^{E\times D}$. Although this has already shown in the lectures, let us review the result $\frac{\partial \textbf{z}}{\partial \textbf{x}}$ using the notation of the course.
\[
z_i = \sum_{j=1}a_{ij}x_j + b_i
\]
We can now easily compute $\frac{\partial z_i}{\partial x_j}$
\[
\frac{\partial z_i}{\partial x_j} = a_{ij}, \quad \frac{\partial \textbf{z}}{\partial \textbf{x}} = A
\]
Let us use all the previous results to compute the derivative of $f(\textbf{z})$ w.r.t. $\textbf{x}$.
\[
\frac{\partial f}{\partial \textbf{x}}= diag(\cos(\textbf{z})) A, \quad \text{where e.g. } diag(\textbf{v}) = \begin{pmatrix}
    v_1 & 0 & \dots & 0 \\
    0 & v_2 & \dots & 0 \\
    \vdots & & \ddots & \vdots \\
    0 & \dots & \dots & v_N \\
\end{pmatrix},\quad \textbf{v}\in\mathbb{R}^N
\]
\[
\frac{\partial f}{\partial \textbf{x}}= diag(\cos(A\textbf{x} + \textbf{b}))A
\]
Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^{E\times D}, \quad \frac{\partial f}{\partial \textbf{z}} \in \mathbb{R}^{E\times E}, \quad \frac{\partial \textbf{z}}{\partial \textbf{x}} \in \mathbb{R}^{E\times D}
\]

    \item $f(z) = \exp(-\frac{1}{2}z), \quad z = g(\textbf{y}) =  \textbf{y}^TS^{-1}\textbf{y}, \quad \textbf{y} = h(\textbf{x}) = \textbf{x} - \bm{\mu}, \quad \textbf{x},\bm{\mu}\in\mathbb{R}^D, S\in\mathbb{R}^{D\times D}$
    
\begin{align*}
\frac{\partial f}{\partial \textbf{x}} &= \frac{\partial f}{\partial z} \frac{\partial z}{\partial \textbf{y}}\frac{\partial \textbf{y}}{\partial \textbf{x}} = \frac{\partial \exp(-\frac{1}{2}z)}{\partial z} \frac{\partial (\textbf{y}^TS^{-1}\textbf{y})}{\partial \textbf{y}}\frac{\partial (\textbf{x} - \bm{\mu})}{\partial \textbf{x}} = \exp\left(-\frac{1}{2}z\right)\left(-\frac{1}{2}\right)\textbf{y}^T(S^T + S^{-T})I \\
&=-\frac{1}{2}\exp\left(-\frac{1}{2}\Big((\textbf{x} - \bm{\mu})^TS^{-1}(\textbf{x} - \bm{\mu})\Big)\right)(\textbf{x} - \bm{\mu})^T(S^T + S^{-T})
\end{align*}
where $S^{-T} = \left(S^{-1}\right)^T$, and we use (5.107) to calculate $\frac{\partial (\textbf{y}^TS^{-1}\textbf{y})}{\partial \textbf{y}}$.

Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^D, \quad \frac{\partial f}{\partial z}\in\mathbb{R}, \quad \frac{\partial z}{\partial \textbf{y}}\in\mathbb{R}^{D}, \quad \frac{\partial \textbf{y}}{\partial \textbf{x}}\in\mathbb{R}^{D\times D}
\]
    
    \item $f(\textbf{x}) = tr(\textbf{x}\textbf{x}^T + \sigma^2I),\quad\textbf{x}\in\mathbb{R}^D$
    
Let us expand $f(x)$.
\begin{align*}
f(x)&= \sum_{i=1}^D \Big( (\textbf{x}\textbf{x}^T)_{ii} + \sigma^2 \Big)\\
    &= \sum_{i=1}^D (\textbf{x}\textbf{x}^T)_{ii} + D\sigma^2 = \sum_{i=1}^D x_i^2 + D\sigma^2
\end{align*} 
We already know that $(\textbf{x}\textbf{x}^T)_{ij} = x_ix_j$. Therefore
\[
\frac{\partial f}{\partial \textbf{x}} = \frac{\partial \left( \sum_{i=1}^D x_i^2 + D\sigma^2 \right)}{\partial \textbf{x}} = 2\textbf{x}^T
\]

    \item $f(\textbf{z}) = \tanh(\textbf{z})\in\mathbb{R}^M,\quad \textbf{z}= A\textbf{x} + \textbf{b}, \quad \textbf{x}\in\mathbb{R}^N, A\in\mathbb{R}^{M\times N}, \textbf{b}\in\mathbb{R}^M$
\begin{align*}
\frac{\partial f}{\partial \textbf{x}} &= \frac{\partial f}{\partial \textbf{z}} \frac{\partial \textbf{z}}{\partial \textbf{x}}= \frac{\partial \tanh(\textbf{z})}{\partial \textbf{z}} \frac{\partial (A\textbf{x} + \textbf{b})}{\partial \textbf{x}} = diag\left(1 - \tanh^2(\textbf{z})\right) \textbf{A}\\
&= diag\left(1 - \tanh^2(A\textbf{x} + \textbf{b})\right) A 
\end{align*}
where we used $\frac{d \tanh(v)}{dv} = 1 - \tanh^2(v)$.

Dimensions are 
\[
\frac{\partial f}{\partial \textbf{x}} \in \mathbb{R}^{M\times N}, \quad \frac{\partial f}{\partial \textbf{z}}\in\mathbb{R}^{M\times M}, \quad \frac{\partial z}{\partial \textbf{x}}\in\mathbb{R}^{M\times N}
\]

\item $f(A) = \textbf{x}\transpose A \textbf{x}, \quad A=\textbf{x}\textbf{x}\transpose, \quad A\in\mathbb{R}^{N\times N}$

Note that $A$ is symmetric. We apply the chain rule straightforwardly.
\begin{align*}
\frac{d f}{d \textbf{x}} &= \frac{\partial f}{\partial \textbf{x}} + \frac{\partial f}{\partial A} \frac{\partial A}{\partial \textbf{x}} = \frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial \textbf{x}} + \sum_{i,j}\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial a_{ij}} \frac{\partial a_{ij}}{\partial\textbf{x}}
\end{align*}

The first term can be computed using vector calculus rules
\begin{align*}
\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial \textbf{x}}  = 2A\textbf{x} = 2\textbf{x}\textbf{x}^T\textbf{x} = 2 ||\textbf{x}||^2\textbf{x}.
\end{align*}
We can compute the second term using index notation, i.e. the derivative with respect to $x_k$.
\begin{align*}
\sum_{i,j}\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial a_{ij}} \frac{\partial a_{ij}}{\partial x_k} &= \sum_{i,j} a_{ij}\left(\delta_{ik}x_j + \delta_{jk}x_i\right) = \sum_{i,j} a_{ij}\delta_{ik}x_j + \sum_{i,j} a_{ij}\delta_{jk}x_i\\
&= \sum_{j} a_{kj}x_j + \sum_{i} a_{ik}x_i = 2 \sum_{i} x_k x_i^2 = 2 ||\textbf{x}||^2x_k.
\end{align*}
Where we used vector calculus rules and \questionref{q:mml55-56} c) to derive the following
\begin{align*}
\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial A}  = \textbf{x}\textbf{x}^T = A, \quad \frac{\partial a_{ij}}{\partial x_k} = \delta_{ik}x_j + \delta_{jk}x_i, \quad a_{ij} = x_ix_j.
\end{align*}
In conclusion, we have
\begin{align*}
\frac{d f}{d \textbf{x}} &= \frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial \textbf{x}} + \sum_{i,j}\left(\frac{\partial (\textbf{x}^T A \textbf{x}) }{\partial a_{ij}}\right) \frac{\partial a_{ij}}{\partial\textbf{x}} = 2 ||\textbf{x}||^2\textbf{x}+ 2 ||\textbf{x}||^2\textbf{x} = 4 ||\textbf{x}||^2\textbf{x}.
\end{align*}

Extra: one can check the previous result is true by simplifying the initial form of $f$, which we maintained for illustrative purposes, i.e. $f(\textbf{x}) = ||\textbf{x}||^4$. We can compute the derivative using index notation
\begin{align*}
\frac{df}{d x_i} = \frac{d}{d x_i}\left(\sum_{i=1}^N x_i^2 \right)^2 = 2||\textbf{x}||^2 2x_i = 4||\textbf{x}||^2 x_i,
\end{align*}
which in vector form is expressed as follows
\begin{align*}
\frac{df}{d x} = 4||\textbf{x}||^2 \textbf{x}.
\end{align*}

\end{enumerate}






\paragraph{\questionref{q:hessian}}
The objective function and gradient w.r.t.~$\vtheta$ (see lectures) for Linear Regression is
\begin{align}
L(\vtheta) = \norm{\vy - \Phi(X)\vtheta}^2 \,, &&
\deriv[L]{\vtheta} = 2(\Phi(X)\vtheta - \vy)\transpose\Phi(X) \,.
\end{align}
We begin by finding the Hessian, i.e.~the matrix containing all second partial derivatives. We need to do this in index notation, as the vector conventions of our vector chain rule break down. So we first write the derivative in index notation, and then we take the derivative again, after which we return to vector notation:
\begin{align}
\pderiv[]{\theta_j} \left(\pderiv[L]{\theta_i}\right) &=
\pderiv[]{\theta_j} \left(2\sum_k\left(\sum_m\Phi_{km}\theta_m - y_k\right)\Phi_{ki}\right) =
\pderiv[]{\theta_j} \left(2\sum_k\left(\sum_m\Phi_{km}\theta_m - y_k\right)\Phi_{ki}\right) \\
&= 2\sum_{km}\Phi_{km}\delta_{mj}\Phi_{ki} = 2\sum_{k}\Phi_{kj}\Phi_{ki} \,, \\
\implies \mat H_{\vtheta}(L) &= 2 \Phi(X)\transpose\Phi(X) \,.
\end{align}
The Hessian doesn't depend on the parameter $\vtheta$, so if we prove that the matrix is positive definite, then the local where $\deriv[L]{\theta} = 0$ (see lecture slides) will be a minimum. For a matrix to be PD, we need $\vv\transpose\mat H\vv > 0$ for all $\vv$. We substitute our Hessian into $\mat H$ to prove this
\begin{align}
\vv\transpose \mat H \vv &= 2\vv\transpose \Phi(X)\transpose\Phi(X) \vv \\
&= \vw\transpose\vw = \sum_i w_i^2\,, && \text{with } \vv = \Phi(X)\vv\,.
\end{align}
This already shows that $\vv\transpose \mat H \vv \geq 0$, with equality if there exists a $\vv$ such that $\Phi(X)\vv = 0$. So now we need to prove that \emph{there cannot be} a $\vv$ for which $\Phi(X)\vv = 0$. If $\mathrm{rank}\,\Phi(X) \geq M$, then this will not happen, by the rank-nullity theorem \citep[\S 2.7.3]{mml}.

At this point, we need to assume this is the case. For full marks though, you should state the implications on the problem at hand, rather than in abstract maths. One \emph{necessary} implication of this is that $N\geq M$. This is only a necessary condition, rather than a sufficient one, since even of $N \geq M$, $\Phi(X)$ can still have many linearly dependent rows. This will at least happen if you observe repeated input points. However, to prove more than this, you need more information about $\Phi(X)$.\footnote{A case that is harder think about is if you observe points that make the feature vectors $\vphi(\vx_n)$ linearly dependent. One example is if you have a 2D input with $\vphi(\vx) = \vx\transpose$, and all your input points lie on a line.}

So to summarise, we could prove that \textbf{if $\mathrm{rank}\,, \Phi(X) \geq M$, which at least needs $N\geq M$, then Linear Regression has a single minimum solution}.

If we are coding up a linear regression problem, and we want to check numerically for a \emph{specific} regression problem whether there is a unique solution, we can compute the eigenvalues of $\Phi(X)\transpose\Phi(X)$, and see if they are all positive. This implies a PD Hessian because
\begin{align}
\vv\transpose \mat H \vv &= \vv\transpose \mat Q \mat \Lambda \mat Q\inv \vv && \text{(eigenvalue decomposition)} \\
&= \vv\transpose \mat Q \mat \Lambda \mat Q\transpose\vv && \text{($\mat H = \mat H\transpose$, so $\mat Q\mat \Lambda \mat Q\inv = (\mat Q\mat \Lambda \mat Q\inv)\transpose$, so $\mat Q\inv = Q\transpose$)}\\
&= \vz\transpose \Lambda \vz \,,
\end{align}
which is only $> 0$ if all the elements in the diagonal matrix $\Lambda$ are positive.

If any of the linear algebra was unfamiliar, I recommend looking at chapter 2 in \citet{mml}, particularly \S2.3, \S2.6, and \S2.7, or the 1st year linear algebra course.


\section{Answers Lecture 3: Automatic Differentiation}



\paragraph{\questionref{q:autodiff-productrule}}
We begin by drawing the computational graph (\cref{fig:qproductrule-compgraph}). % We make the distinction between $v_i$ and the functions to highlight the difference between an \emph{evaluation} of a function, and the function itself. All the $v_i$s refer to specific evaluations, and are therefore numbers.
We now find the primal trace and the forward tangent trace:
\begin{align}
v_0 &= x && \pderiv[v_0]{x} = 1 \\
v_1 &= a(x) && \pderiv[v_1]{x} = \pderiv[v_1]{v_0}\pderiv[v_0]{x} = \pderiv[a(x)]{x} \\
v_2 &= b(x) && \pderiv[v_2]{x} = \pderiv[v_2]{v_0}\pderiv[v_0]{x} = \pderiv[b(x)]{x} \\
v_3 &= v_1 \cdot v_2 && \pderiv[v_3]{x} = \sum_{j \in \mathrm{inputs}(3)}\pderiv[v_3]{v_j}\pderiv[v_j]{x} = v_2\pderiv[a(x)]{x} + v_1\pderiv[b(x)]{x} \,.
\end{align}
This means that for any x, forward mode autodiff calculates the derivative to be:
\begin{align}
\deriv[f]{x} = b(x)\deriv[a(x)]{x} + a(x)\deriv[b(x)]{x} \,.
\end{align}
Which is the product rule.

If we substitute $a(x) = x, b(x) = x$, then we obtain $\calcd f / \calcd x = 2x$, as expected.


\begin{figure}[t]
\centering
  \tikz{
 \node[const] (x) {$x$};%
 \node[latent, right=of x] (v0) {$v_0$};%
 \node[latent, right=of v0, yshift=0.6125cm] (v1) {$v_1$};
 \node[latent, right=of v0, yshift=-0.6125cm] (v2) {$v_2$};
 \node[latent, right=of v1, yshift=-0.6125cm] (v3) {$v_3$};
 \node[const, right=of v3] (out) {out};
 \edge {x}  {v0};
 \edge {v0} {v1,v2};
 \edge {v1,v2}  {v3};
 \edge {v3}     {out};
}
\caption{Computational graph for \questionref{q:autodiff-productrule}.}
\label{fig:qproductrule-compgraph}
\end{figure}



\paragraph{\questionref{q:autodiff}}
\begin{figure}[h]
\centering
  \tikz{
 \node[const] (l) {$\boldsymbol\ell$};%
 \node[const,  below=of l] (X) {$\mat X$};%
 \node[latent, right=of X] (v0) {$v_0$};%
 \node[latent, right=of v0] (D1) {$D_1$};
 \node[latent, right=of v0, yshift=-1.25cm] (D2) {$D_2$};
 \node[latent, right=of D1, yshift=0.6125cm] (L1) {$\Lambda_1$};
 \node[latent, right=of D2, yshift=0.6125cm] (L2) {$\Lambda_2$};
 \node[latent, above=of D1,yshift=-0.4333cm] (vn1) {$v_{-1}$};%
 \node[latent, right=of L1] (K1) {$K_1$};
 \node[latent, right=of L2] (K2) {$K_2$};
 \node[latent, right=of K2, yshift=0.6125cm] (K)  {$v_1$};
 \node[latent, right=of K] (Kinv)  {$v_2$};
 \node[latent, right=of Kinv] (v3)  {$v_3$};
 \node[const, right=of v3] (out)  {out};
 % \node[latent, right=of v0, yshift=0.6125cm] (v1) {$v_1$};
 % \node[latent, right=of v0, yshift=-0.6125cm] (v2) {$v_2$};
 % \node[latent, right=of v1, yshift=-0.6125cm] (v3) {$v_3$};
 % \node[const, right=of v3] (out) {out};
 \edge {l}  {vn1};
 \edge {X}  {v0};
 \edge {v0} {D1};
 \edge {v0}  {D2};
 \edge {D1} {L1};
 \edge {D2} {L2}; 
 \edge {vn1} {L1,L2};
 \edge {L1} {K1};
 \edge {L2} {K2}; 
 \edge {K1, K2} {K};
 \edge {K} {Kinv};
 \edge {Kinv} {v3};
 \edge {v3} {out};
}
\caption{Computational graph for \questionref{q:autodiff}, where we define $v_1 = \mat K_1 + \mat K_2$, $v_2 = v_1\inv$, and $v_3 = \vy\transpose v_1 \vy$.}
\label{fig:qautodiff}
\end{figure}
\begin{enumerate}[label=\alph*.]
\item
\begin{align}
D_a &: \Reals^{N\times 2} &&\to \Reals^{N\times N} \\
\Lambda_a &: \Reals^{N\times N}\times \Reals^{2} &&\to \Reals^{N\times N} \\
K_a &: \Reals^{N\times N} &&\to \Reals^{N\times N} \\
f &: \Reals^{N\times N}\times\Reals^{N\times N} &&\to \Reals
\end{align}
\item See \cref{fig:qautodiff}. We use names evident from the question for some nodes, but give new names to some additional intermediate notes.
\item Let's first consider \textbf{forward mode} for the derivatives w.r.t.~$\mat X$.
\begin{align}
v_{-1} &= \boldsymbol \ell && \dot v_{-1,iab} = \pderiv[{[v_{-1}]}_{i}]{X_{ab}} = 0 && v_{-1} \in \Reals^{2 \times (N\times 2)}\,, O(N) \\ % \pderiv[v_{-1}]{\boldsymbol\ell} = \mat I_2
v_0 &= X && \dot v_0 = \pderiv[{[v_0]}_{ij}]{X_{ab}} = \delta_{ia}\delta_{jb}  &&  \dot v_0 \in \Reals^{(N\times 2)\times (N\times 2)}\,, O(N^2) \\
D_z &= \dots && \pderiv[{[D_z]}_{nm}]{v_{0ij}} = \pderiv{v_{0ij}} (v_{0nz} - v_{0mz})^2 &&  \dot D_z \in \Reals^{(N\times N) \times (N\times 2)}  \nonumber \\
& && \qquad = 2(v_{0nz} - v_{0mz})(\delta_{ni}\delta_{zj} - \delta_{mi}\delta_{zj}) &&\\
& && \dot D_{znmab} = \left[\pderiv[D_z]{v_0} \dot v_0\right]_{nmab} && O(N) \text{ for sum, so total } O(N^4). \\
& && \quad = 2 (v_{0nz} - v_{0mz}) (\dot v_{0nzab} - \dot v_{0mzab})&& \text{Structure allows } O(N^3). \\
\Lambda_z &= - \frac{D_z}{2v_{-1z}^2} && \pderiv[\Lambda_{zij}]{D_{znm}} = - \frac{D_z}{2v_{-1z}^2}\delta_{in}\delta_{jm}  && \dot \Lambda_z \in \Reals^{(N\times N) \times (N\times 2)} \\
& && \pderiv[\Lambda_{zij}]{v_{-1,k}} = \frac{D_{zij}}{v_{-1z}^3} \delta_{zk} && \\
& && \dot \Lambda_{zijab} = \left[\pderiv[\Lambda_{z}]{D_{z}} \dot D_{z} + \pderiv[\Lambda_{z}]{v_{-1}} \dot v_{-1}\right]_{ijab} && O(N^2)\text{ for sum, so total }O(N^5)\,.
\end{align}
\item {\color{red} To be continued...}
\end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "exercises"
%%% End: 
