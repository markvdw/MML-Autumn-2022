%% Time-stamp: <2018-10-18 20:24:12 (marc)>
\documentclass[xcolor=x11names,compress,mathserif,handout]{beamer}

\newcommand{\hackspace}{\hspace{4.2mm}}
\newcommand{\showstudent}[1]{}
\newcommand\hmmax{0}
\newcommand\bmmax{0}


\usepackage{../includes/MarkMathCmds}





% talk/author information
\newcommand{\authorname}{Mark van der Wilk}
\newcommand{\authoremail}{m.vdwilk@imperial.ac.uk}
\newcommand{\authoraffiliation}{
  Department of Computing\\Imperial
  College London}
\newcommand{\authortwitter}{markvanderwilk}
\newcommand{\slidesettitle}{\imperialBlue{Vector Calculus}}
\newcommand{\footertitle}{Differentiation}
\newcommand{\location}{Imperial College London}
\newcommand{\talkDate}{October 12, 2021}



\date{\imperialGray{\talkDate}}




% load defaults
\input{../includes/header.tex}


\input{../includes/titlepage.tex}
\linespread{1.2} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Reading Material}
  \begin{center}
    Lecture notes, Chapter 5\\
     \emph{\url{https://mml-book.com}}
  \end{center}
\end{frame}


\section{Optimisation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
 \frametitle{Curve Fitting (Regression) in Machine Learning (2)}

 \begin{figure}
   \centering
   \includegraphics[width = 0.4\hsize]{./figures-intro-curvefitting/polynomial5}
 \end{figure}
 
 \vspace{-1mm}
 \begin{itemize}
 \item \cemph{Training the model} means finding parameters $\vec\theta^*$, such
   that $f(\vec x_i,\vec\theta^*)\approx y_i$
 \item Define a \cemph{loss function}, e.g., $\sum_{i=1}^N(y_i - f(\vec x_i,
   \vec\theta))^2$, which we want to optimize% \\
   % \arrow \cemph{Maximum likelihood estimation} does this implicitly
 \item Adjust $\vec\theta$ until loss is as small as we can get it: \emph{Minimisation} / \emph{optimisation}.
 \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Minimising the loss}
 \begin{figure}
   \centering
   \only<1>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-problem.png}}
   \only<2>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-0.png}}
   \only<3>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-1.png}}
   \only<4>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-2.png}}
   \only<5>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-3.png}}
   \only<6>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-4.png}}
   \only<7>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-5.png}}
   \only<8>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-6.png}}
   \only<9>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-7.png}}
   \only<10>{\includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-8.png}}
 \end{figure}
\begin{align}
f(x) = a\cdot x && L(a) = \sum_{n=1}^N (f(x_n, a) - y_n)^2
\end{align}
\begin{center}
Let's adjust $a$.
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example: Minimising the loss}
 \begin{figure}
   \centering
   \includegraphics[width=\hsize]{./figures-vectorcalc/linreg-loss-2.png}
 \end{figure}
Two questions for now:
\begin{itemize}
\item How should we change $a$ to make the loss smaller?
\item How do we know when we can't get better?
\end{itemize}
\end{frame}





% \begin{frame}
% \frametitle{Types of Differentiation}

% \begin{enumerate}
% \item Scalar differentiation: $f:\R\to\R$ \\ \qquad \qquad \colchar{$y \in \R \text{ w.r.t. } x \in \R$}{green}
% \item Multivariate case: $f:\R^N\to\R$ \\ \qquad \qquad \colchar{$y \in \R \text{ w.r.t. vector } \vec x \in \R^N$}{green}
% \item Vector fields: $f:\R^N\to\R^M$ \\ \qquad \qquad \colchar{$\text{vector } \vec y \in \R^M \text{ w.r.t. vector } \vec x \in \R^N$}{green}
% \item General derivatives: $f:\R^{M\times N}\to \R^{P\times Q}$ \\
%   \qquad \qquad \colchar{$\text{matrix } \vec y \in \R^{P \times Q}
%     \text{ w.r.t. matrix }\mat X \in \R^{M \times N}$}{green}
% \end{enumerate}


% \end{frame}


\section{Differentiation w.r.t.~scalars}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Scalar Differentiation $f: \R\to\R$}

% \begin{figure}
%   \centering
%   \includegraphics[width = 0.6\hsize]{./figures/finite_differences}
% \end{figure}

\begin{itemize}
\item Derivative defined as the limit of the difference quotient
%
$$
f^\prime(x) = \frac{df}{dx} = \lim_{h\to 0} \frac{f(\colchar{$x + h$}{red}) - f(x)}{h}
$$
\arrow Slope of the secant line through $f(x)$ and
$f(x+h)$
\end{itemize}
\begin{center}
  \animategraphics[loop,
  width=0.6\hsize]{10}{./figures-vectorcalc/finite-diff-animation/finite_diff_step_}{99}{0}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Some Examples}
$$
\begin{array}{ll}
  f(x) = x^n\qquad & f^\prime(x) = nx^{n-1}\\
  f(x) = \sin(x)\qquad & f^\prime(x) = \cos(x)\\
  f(x) = \tanh(x) \qquad& f^\prime(x) = 1-\tanh^2(x)\\
  f(x) = \exp(x) \qquad &f^\prime(x) = \exp(x)\\
  f(x) = \log(x) \qquad &f^\prime(x) = \frac{1}{x}
\end{array}
$$
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Differentiation Rules}

  \begin{itemize}[<+->]
  \item Sum Rule
    $$\big(f(x)+g(x)\big)^\prime = \green{f^\prime(x)} + \blue{g^\prime(x)} =
    \green{\frac{df(x)}{dx}} + \blue{\frac{dg(x)}{dx}}$$
  \item Product Rule
    $$
    \big(f(x) g(x)\big)^\prime = \green{f^\prime(x)g(x)} + \blue{f(x)g^\prime(x)} =
    \green{\frac{df(x)}{dx}g(x)} + \blue{f(x) \frac{d g(x)}{dx}}
    $$
  \item Chain Rule
    $$
     (g\circ f)^\prime(x) =  \big(g(f(x))\big)^\prime
     =\green{g^\prime(f(x))}\blue{f^\prime(x)} = \green{\frac{dg(f(x))}{df}}\blue{\frac{df(x)}{dx}}
    $$
   \item Quotient Rule
   	$$
   	\Big(\frac{f(x)}{g(x)}\Big)^\prime = \frac{\green{f(x)^\prime g(x)} - \blue{f(x)g(x)^\prime}}{\green{(g(x))^2}} = \frac{\blue{\frac{df}{dx}g(x)} - \green{f(x)\frac{dg}{dx}}}{\green{(g(x))^2}}
   	$$
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Scalar Chain Rule}
$$
     (g\circ f)^\prime(x) =  \big(g(f(x))\big)^\prime
     =\green{g^\prime(f(x))}\blue{f^\prime(x)} = \green{\frac{dg}{df}}\blue{\frac{df}{dx}}
    $$
    
\begin{columns}[t]

\column{0.50\hsize}
\begin{center}
\textbf{Beginner}
\end{center}
\vspace{-6mm}

\begin{align*}
  g(z) &= 6z + 3\\
  z &= f(x) = -2x + 5 \\
  (g\circ f)^\prime(x) &= \onslide+<2->{\green{\underbrace{(6)}_{dg/df}}\blue{\underbrace{(-2)}_{df/dx}} \\ &= -12}
\end{align*}


\column{0.60\hsize}
\begin{center}
\textbf{Advanced}
\end{center}
\vspace{-6mm}

\begin{align*}
%  f(x) = \frac{\exp(x) + \exp(-x)}{\exp(x)}\\
  g(z) &= \tanh(z)\\
  z &= f(x) = x^n \\
  (g\circ f)^\prime(x) &= \onslide+<2->{\green{\underbrace{\big(1-\tanh^2(x^{n}))}_{dg/df}}\blue{\underbrace{nx^{n-1}}_{df/dx}}}
\end{align*}

\end{columns}
\onslide+<1>{
\vspace{-4mm}
\begin{center}
\textbf{Work it out with your neighbors}
\end{center}}
\end{frame}



\begin{frame}{Finding minima}
\vspace{-0.4cm}
 \begin{figure}
   \centering
   \includegraphics[width=0.85\hsize]{./figures-vectorcalc/gradient-example.png}
 \end{figure}
\vspace{-0.4cm}
Q1: How should we change the input to reduce the output? \pause \\
% The derivative tells us how the function changes if we increase the input. So: \pause
\begin{itemize}
\item Find the derivative function and compute it at a point to find the point's gradient. \pause
\item Increase for negative gradients. Decrease for positive gradients. \pause
\end{itemize}
This is the idea behind \emph{gradient descent}.
\end{frame}



\begin{frame}[allowframebreaks]{Finding minima}
\vspace{-0.4cm}
 \begin{figure}
   \centering
   \only<1,3>{\includegraphics[width=0.85\hsize]{./figures-vectorcalc/gradient-example.png}}
   \only<2>{\includegraphics[width=0.85\hsize]{./figures-vectorcalc/gradient-example-max.png}}
 \end{figure}
\vspace{-0.4cm}
\begin{itemize}
\item At a minimum, there is no change we can make that lowers our function value $\implies$ gradient must be zero. \pause
\item Zero gradient is not enough! \pause
\item For minimum, $f(x)$ must go from decreasing to increasing \\
$\implies$ gradient of gradient positive
\end{itemize}
\end{frame}


\begin{frame}{Local and global minima}
Board.
\end{frame}



\begin{frame}{Example: Linear regression}
For the example from earlier, find optimal $a$:
\begin{align}
f(x) = a\cdot x && L(a) = \sum_{n=1}^N (f(x_n) - y_n)^2
\end{align} \pause
\begin{align}
\deriv[L]{a} = \sum_{n=1}^N 2 (a x_n - y_n) x_n = \sum_{n=1}^N 2ax_n^2 - 2x_ny_n = 0
\end{align} \pause
\begin{align}
2a \sum_n x_n^2 = \sum_n2x_ny_n
\end{align} \pause
\begin{align}
a = \frac{\sum_nx_ny_n}{\sum_n x_n^2}
\end{align} \pause
\begin{align}
\frac{\mathrm{d}^2L}{\mathrm{d}a^2} = \sum_{n=1}^N 2x_n^2 \geq 0
\end{align}
\end{frame}


\begin{frame}{Summary}
You have seen:
\begin{itemize}
\item That derivatives are useful for finding minima of functions \pause
\item How to differentiate simple functions \pause
\item An example of solving for the minimum point \pause
\item How to identify minima
\end{itemize}

% Consider exercises 5.1-5.3 in the MML book. EXERCISES
\end{frame}

\section{Differentiation w.r.t.~vectors}

\begin{frame}{Linear regression: multiple parameters}
What happens when our function has multiple parameters?
\begin{equation}
f(x) = \theta_3 x^3 + \theta_2 x^2 + \theta_1 x + \theta_0
\end{equation} \pause

Think of a \emph{vector} as parameterising our function:
\begin{align}
f(x) = \vec\theta\transpose \vphi(x) && \vphi(x) = \begin{bmatrix}x^3 && x^2 && x && 1\end{bmatrix}\transpose
\end{align} \pause

We want to:
\begin{itemize}
\item Understand how a function (e.g.~loss) changes when we change $\vec\theta$. \pause
\item Characterise what an optimum is for a function of a vector.
\end{itemize} \pause

\begin{center}
Both can be analysed by \\ turning the multi-D problem into many 1D problems.
\end{center}
\end{frame}

\begin{frame}{TODO}
Todo for next year: State the question clearer, with mathematical notation. What we want from vector differentiation.
\end{frame}

\begin{frame}{Directional derivative}

 \begin{figure}
   \centering
   \includegraphics[width=0.7\hsize]{./figures-vectorcalc/directional-deriv.png}
 \end{figure}

 How does the function change if we move in a particular \textit{direction}?
\end{frame}

\begin{frame}{Directional derivative}
Define \emph{directional derivative} $\nabla_\vv L(\vtheta)$ as how much the function changes if we move in direction $\vv$:
\only<1>{
\begin{align*}
\nabla_\vv L(\vtheta) = \lim_{h\to 0} \frac{L(\vtheta + h\vv) - L(\vtheta)}{h}
\end{align*}}
\only<2->{\begin{equation*}
\nabla_\vv L(\vtheta) = \lim_{h\to 0} \frac{L(\theta_1 + hv_1, \theta_2 + hv_2) - L(\theta_1, \theta_2)}{h}
\end{equation*}} \pause\pause
\begin{equation*}
= \lim_{h\to 0} \frac{L(\theta_1 + hv_1, \theta_2 + hv_2) - {\color{blue}L(\theta_1, \theta_2  + hv_2)}}{h} + \frac{{\color{blue}L(\theta_1, \theta_2 + hv_2)} - L(\theta_1, \theta_2)}{h}
\end{equation*}\pause
\begin{equation*}
= \lim_{h\to 0} \frac{L(\theta_1 + h', \theta_2 + h'\frac{v_2}{v_1})\!-\!{\color{blue}L(\theta_1, \theta_2  + h'\frac{v_2}{v_1})}}{h'/v_1}\!+\! \frac{{\color{blue}L(\theta_1, \theta_2 + h'')}\!-\!L(\theta_1, \theta_2)}{h''/v_2}
\end{equation*}\pause
\begin{equation*}
= \pderiv[L]{\theta_1} v_1 + \pderiv[L]{\theta_2} v_2% && \qquad \text{to prove, substitute e.g.~}h' = hv_1
\end{equation*} \pause
\begin{itemize}
\item Can find gradient in \emph{any} direction with the \emph{partial derivatives}
% \item An optimum is where the gradient is zero in \emph{all directions}!
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Multivariate Differentiation $f: \R^N\to\R$}

  \begin{align*}
    y = f(\vec x)\,,\quad \vec x =\begin{bmatrix}
                                    x_1\\
                                    \vdots\\
                                    x_N
                                  \end{bmatrix}\in\R^N
  \end{align*}

  \begin{itemize}[<+->]
  \item \cemph{Partial derivative} (change one coordinate at a time):
    \begin{align*}
      \frac{\partial f}{\partial x_i} = \lim_{h\to 0}
      \frac{f(x_1,\dotsc, x_{i-1}, \colchar{$x_i + h$}{red}, x_{i+1},
      \dotsc, x_N) - f(\vec x)}{h}
    \end{align*}
  \item \cemph{Jacobian} vector (\cemph{gradient}) collects all partial derivatives:
    \begin{align*}
      \diffF{f}{\vec x} =
      \begin{bmatrix}
         \frac{\partial f}{\partial x_1} & \cdots & \pdiffF{f}{x_N}
      \end{bmatrix}\in\R^{1\times N}
    \end{align*}
    Note: By convention, we define this to be a \emph{row vector}.
  \end{itemize}
  
\end{frame}


\begin{frame}{Multivariate Differentiation $f: \R^N\to\R$}
\begin{myblock}{Derivative w.r.t.~vector}
Since we can find the directional derivative \textit{in any direction} with the Jacobian, we \emph{define} this vector to be the derivative of a function w.r.t.~a vector.
\end{myblock}
\end{frame}



\begin{frame}{Steepest descent direction}
Directional derivative:
\begin{equation}
\nabla_{\vec v} f(\vtheta) = \diffF{f}{\vtheta}\vec v
\end{equation}
\vspace{-0.5cm}
\begin{center}\small(inner product, row vector times column vector)\end{center} \pause

\vspace{0.2cm}

\begin{center}
\large What is the direction where the function changes the most?
\end{center} \pause
\begin{equation}
\diffF{f}{\vtheta}\vec v = \norm{\diffF{f}{\vtheta}}\norm{\vec v} \cos \beta 
\end{equation}\pause
\begin{itemize}
\item Choose unit vector $\vec v$ \pause
\item Angle between vectors $\beta$ should be zero $\implies \cos \beta = 1$. \pause
\end{itemize}
\begin{center}
\Large Steepest descent points in direction of Jacobian/gradient vector.
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
\frametitle{Example: Multivariate Differentiation}

\vspace{-1cm}
\begin{columns}[t]

\column{0.50\hsize}
\begin{center}
\textbf{Beginner}
\end{center}
\vspace{-6mm}

\begin{align*}
  &f: \R^2 \to \R\\
  &f(x_1,x_2) = x_1^2x_2+x_1x_2^3\in\R
\end{align*}

\column{0.50\hsize}
\begin{center}
\textbf{Advanced}
\end{center}
\vspace{-6mm}

\begin{align*}
 &f: \R^2 \to \R\\
  &f(x_1,x_2) = (x_1 + 2x_2^3)^2\in\R
\end{align*}

\end{columns}

\begin{center}
Partial derivatives\visible<1>{? Gradient?}  \\
\visible<1>{\textbf{Work it out with your neighbors}}
\end{center}
\vspace{-1.35cm}

\pause
\begin{columns}[t]

\column{0.5\hsize}
\begin{align*}
\frac{\partial f(x_1,x_2)}{\partial x_1} &= 2x_1x_2 + x_2^3\\
\frac{\partial f(x_1,x_2)}{\partial x_2} &= x_1^2+3x_1x_2^2
\end{align*}

\column{0.5\hsize}
\vspace{-4mm}
\begin{align*}
\frac{\partial f(x_1,x_2)}{\partial x_1} &= 2(x_1 + 2x_2^3)\green{\overbrace{(1)}^{\pdiffF{}{x_1}(x_1 + 2x_2^3)}}\\
\frac{\partial f(x_1,x_2)}{\partial x_2} &= 2(x_1 + 2x_2^3)\blue{\underbrace{(6x_2^2)}_{\pdiffF{}{x_2}(x_1 + 2x_2^3)}}\\
\end{align*}

\end{columns}

\pause
\vspace{-1cm}
\begin{align*}
\text{Gradient}\qquad
\diffF{f}{\vec x} =
  \begin{bmatrix}
\frac{\partial f(x_1,x_2)}{\partial x_1} & \frac{\partial
  f(x_1,x_2)}{\partial x_2}
\end{bmatrix} \in\R^{1\times 2}\,
\end{align*}
\vspace{-1.35cm}

\begin{columns}[t]

\column{0.5\hsize}
\begin{align*}
  \diffF{f}{\vec x} =
\begin{bmatrix}
 2x_1x_2 + x_2^3 & x_1^2+3x_1x_2^2
\end{bmatrix}
\end{align*}

\column{0.5\hsize}
\begin{align*}
  \diffF{f}{\vec x} =
\begin{bmatrix}
 2(x_1 + 2x_2^3) & 12(x_1 + 2x_2^3)x_2^2
\end{bmatrix}
\end{align*}

\end{columns}

\end{frame}


\begin{frame}{Optima, minima, maxima}
What is an optimum for a function of a vector?
 \begin{figure}
   \centering
   \includegraphics[width=0.5\hsize]{./figures-vectorcalc/directional-deriv.png}
 \end{figure} \pause
\begin{itemize}
\item Directional derivative should be zero \textit{in all directions} $\implies \diffF{f}{\vec x} = {\vec 0}$. \pause
\item For minimum: second directional derivative should be positive \textit{in all directions}.
\end{itemize}
\end{frame}


\begin{frame}{Summary}
Motivation: Want to optimise functions of several variables \\
\vspace{0.2cm}

% Q1: Understand how function changes when we change vector input. \\
% A1: Directional derivative, found from partial derivatives \\
% \vspace{0.2cm}

% Q2: 
\begin{itemize}
\item Directional derivative
\item Partial derivatives $\implies$ gradient vector
\item Steepest descent direction
\item At an optimum $\diffF{f}{\vec x} = \vec 0$
\end{itemize} \pause

\vspace{0.5cm}

Next time: Derivatives of vectors and chain rules.
\end{frame}





\end{document}

