\section{Answers Lectures 6 \& 7: Multivariate Probability}

\paragraph{\questionref{q:vector-independence}}
\begin{enumerate}[label=\alph*.]
\item Parts you should justify: Why is there no different normalising constant? You should use the change-of-variables formula to get to this answer. Answer:
\begin{align}
p(\tilde \vx) = c \cdot \tilde \vx\transpose \begin{bmatrix}0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 0 & 0\end{bmatrix} \tilde \vx
\end{align}
\item
\begin{align}
p(\vx) &= c \cdot \begin{bmatrix}x_2 + x_4 & 0 & x_2 + x_4 & 0\end{bmatrix}\begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4\end{bmatrix} \\
&= c \cdot x_1 (x_2+x_4) + x_3(x_2 +x_4) \\
&= c \cdot (x_1 + x_3)(x_2+x_4) \\
&= p(\vz)p(\vy)
\end{align}
\end{enumerate}

\paragraph{Question \ref{q:linear_transform_gaussian}}
$Y$ is distributed as a multivariate Gaussian with mean $\BA \bm{\mu}$ and covariance matrix $\BA \Sigma \BA^\top$.

To see this, we first define $T(X) = \BA X$, so that we have $Y = T(X)$. Using change of variables rule, the PDF of $Y$ needs to satisfy the following:
$$p_Y(\y) = p_X(T^{-1}(\y)) | \frac{dT^{-1}(\y)}{dy} | .$$
As $T^{-1}(\y) = \BA^{-1} \y$, we have $| \frac{dT^{-1}(y)}{dy} | = | \BA^{-1} | = | \BA |^{-1}$. Plugging in these results:
\begin{equation*}
\begin{aligned}
p_Y(\y) &=  |\BA |^{-1} \frac{1}{\sqrt{(2 \pi)^d |\Sigma|}} \exp[-\frac{1}{2} (\BA^{-1} \y - \bm{\mu})^\top \Sigma^{-1} (\BA^{-1} \y - \bm{\mu})] \\
& = \frac{1}{\sqrt{(2 \pi)^d |\BA \Sigma \BA^\top|}} \exp[-\frac{1}{2} (\y - \BA \bm{\mu})^\top (\BA \Sigma \BA^{\top})^{-1} ( \y - \BA \bm{\mu})].
\end{aligned}
\end{equation*}

\paragraph{Question \ref{q:sum_of_gaussians}}

The PDF of $Z = X + Y$ needs to satisfy the following:
$$p_Z(z) = \int_{x+y = z} p_X(x)p_Y(y) dx dy.$$
Expanding the right hand side expression, we have (assuming the mean and variance of $X, Y$ are $\mu_X, \sigma^2_X, \mu_Y, \sigma^2_Y$):
\begin{equation*}
\begin{aligned}
p_Z(z) &= \int_{x+y = z} p_X(x)p_Y(y) dx dy \\
&= \int_{x \in \mathbb{R}} p_X(x)p_Y(z - x) dx \\
&= \int \frac{1}{\sqrt{2\pi \sigma^2_X}} \exp[-\frac{1}{2\sigma^2_Y}(x - \mu_X)^2]  \frac{1}{\sqrt{2\pi \sigma^2_Y}} \exp[-\frac{1}{2\sigma^2_Y}(z - x - \mu_Y)^2] dx \\
&= \int \frac{1}{2 \pi \sqrt{\sigma^2_X \sigma^2_Y}} \exp[-\frac{1}{2}[ (\sigma_X^{-2} + \sigma_Y^{-2}) x^2 - 2 x (\sigma_X^{-2} \mu_X + \sigma_Y^{-2}(z - \mu_Y)) + \sigma_X^{-2} \mu_X^2 + \sigma_Y^{-2} (z - \mu_Y)^2 ]] dx.
\end{aligned}
\end{equation*}
Define two new parameters $\mu, \sigma$ satisfying $\sigma^{-2} = \sigma_X^{-2} + \sigma_Y^{-2}$ and $\sigma^{-2} \mu = \sigma_X^{-2} \mu_X + \sigma_Y^{-2}(z - \mu_Y)$, we have
\begin{equation*}
\begin{aligned}
p_Z(z) &= \int \frac{1}{2 \pi \sqrt{\sigma^2_X \sigma^2_Y}} \exp[-\frac{1}{2}[ \sigma^{-2} (x - \mu)^2 + \sigma_X^{-2} \mu_X^2 + \sigma_Y^{-2} (z - \mu_Y)^2 - \sigma^{-2} \mu^2 ]] dx \\
&= \int \frac{1}{\sqrt{2 \pi \sigma^2}} \exp[-\frac{1}{2 \sigma^2} (x - \mu)^2] dx \frac{\sqrt{2 \pi \sigma^2}}{2 \pi \sqrt{\sigma^2_X \sigma^2_Y}} \exp[-\frac{1}{2}[ \sigma_X^{-2} \mu_X^2 + \sigma_Y^{-2} (z - \mu_Y)^2 - \sigma^{-2} \mu^2 ]] \\
&= \frac{1}{\sqrt{2 \pi \sigma^2_X \sigma^2_Y \sigma^{-2}}} \exp[-\frac{1}{2}[ \sigma_X^{-2} \mu_X^2 + \sigma_Y^{-2} (z - \mu_Y)^2 - \sigma^2 [\sigma_X^{-2} \mu_X + \sigma_Y^{-2}(z - \mu_Y)]^2 ]]. 
\end{aligned}
\end{equation*}
Up to this point, we see that the PDF of $Z$ has a Gaussian form (as the terms inside the exponent is a quadratic function of $z$), so this confirms that $Z$ is distributed as a univariate Gaussian. Continuing the derivation by completing the squares, we can work out the corresponding mean and variance of $Z$:
$$\sigma_Z^2 = \sigma_X^2 + \sigma_Y^2, \quad \mu_Z = \mu_X + \mu_Y.$$


\paragraph{Question \ref{q:kl_change_of_variable}}
Discrete case: by using $x = T^{-1}(y)$,
\begin{equation*}
\begin{aligned}
\mathrm{KL}[p_Y(y) || q_Y(y)] &= \sum_{y} p_Y(y) \log \frac{p_Y(y)}{q_Y(y)} = \sum_{y} p_X(T^{-1}(y)) \log \frac{p_X(T^{-1}(y))}{q_X(T^{-1}(y))} \\
&=  \sum_{x} p_X(x) \log \frac{p_X(x)}{q_X(x)} = \mathrm{KL}[p_X(x) || q_X(x)].
\end{aligned}
\end{equation*}

Continuous case: again by using $x = T^{-1}(y)$,
\begin{equation*}
\begin{aligned}
\mathrm{KL}[p_Y(y) || q_Y(y)] &= \int p_Y(y) \log \frac{p_Y(y)}{q_Y(y)} dy = \int p_X(T^{-1}(y)) | \frac{d T^{-1}(y)}{dy} | \log \frac{p_X(T^{-1}(y)) | \frac{d T^{-1}(y)}{dy} |}{q_X(T^{-1}(y)) | \frac{d T^{-1}(y)}{dy} |} dy \\
&=  \int p_X(T^{-1}(y)) | \frac{dx}{dy} | \log \frac{p_X(T^{-1}(y)) }{q_X(T^{-1}(y)) } dy = \int p_X(x) \log \frac{p_X(x)}{q_X(x)} dx = \mathrm{KL}[p_X(x) || q_X(x)].
\end{aligned}
\end{equation*}

\paragraph{Question \ref{q:independence_of_gaussians}}
As we define $\Lambda = \Sigma^{-1}$, the PDF of the multivariate Gaussian is (with mean $\bm{\mu}$):
\begin{equation*}
\begin{aligned}
p(X = \x) &= \frac{1}{\sqrt{ (2 \pi)^d | \Sigma |}} \exp [ -\frac{1}{2} (\x - \bm{\mu})^\top \Lambda (\x - \bm{\mu})^\top ] \\
&= \frac{1}{\sqrt{ (2 \pi)^d | \Sigma |}} \exp [ -\frac{1}{2} \sum_k \sum_l (x_k - \mu_k) \Lambda_{kl} (x_l - \mu_l) ] \\
&= \frac{1}{\sqrt{ (2 \pi)^d | \Sigma |}} \exp [ -\frac{1}{2} [ f(x_i) + g(x_j) + \sum_{k \neq i, j} \sum_l (x_k - \mu_k) \Lambda_{kl} (x_l - \mu_l) ] ], \\
\text{where} \quad f(x_i) &= \Lambda_{ii} (x_i - \mu_i)^2 + (x_i - \mu_i) \sum_{l \neq i. j} \Lambda_{il} (x_l - \mu_l), \\
g(x_j) &= \Lambda_{jj} (x_j - \mu_j)^2 + (x_j - \mu_j) \sum_{l \neq i, j} \Lambda_{jl} (x_l - \mu_l).
\end{aligned}
\end{equation*}
%
Now to show $X_i \ci X_j | X_{-ij}$, we need to compute the marginal distribution $p(X_{-ij})$. This can be done using the sum rule:
\begin{equation}
\begin{aligned}
p(X_{-ij}) &= \int p(X_{-ij}, X_i = x_i, X_j = x_j) d x_i d x_j \\
&= \frac{1}{\sqrt{ (2 \pi)^d | \Sigma |}} \exp [ -\frac{1}{2} [ \sum_{k \neq i, j} \sum_l (X_k - \mu_k) \Lambda_{kl} (X_l - \mu_l) ] ] \int \exp [ -\frac{1}{2} [ f(x_i) + g(x_j) ] ] d x_i d x_j.
\end{aligned}
\end{equation}
Without explicitly computing the integral and instead denoting it as $\int \exp [ -\frac{1}{2} [ f(x_i) + g(x_j) ] ] d x_i d x_j = F(X_{-ij})$, we have
$$p(X_i, X_j | X_{-ij}) = \frac{P(X)}{P(X_{-ij})} = \frac{ \exp [ -\frac{1}{2} [ f(X_i) + g(X_j) ] ] }{F(X_{-ij})}.$$
Using the sum rule again, we can show that:
$$p(X_i | X_{-ij}) = \int p(X_i, X_j = x_j | X_{-ij}) d x_j = \frac{ \exp [ -\frac{1}{2} f(X_i) ] }{F(X_{-ij})} \int \exp[-\frac{1}{2} g(x_j)] d x_j.$$
$$p(X_j | X_{-ij}) = \int p(X_i = x_i, X_j | X_{-ij}) d x_j = \frac{ \exp [ -\frac{1}{2} g(X_j) ] }{F(X_{-ij})} \int \exp[-\frac{1}{2} f(x_i)] d x_i.$$
Lastly, notice that:
$$F(X_{-ij}) = \int \exp [ -\frac{1}{2} [ f(x_i) + g(x_j) ] ] d x_i d x_j = \int \exp[-\frac{1}{2} f(x_i)] d x_i \int \exp[-\frac{1}{2} g(x_j)] d x_j.$$
Combining all the above equations proves that
$$p(X_i, X_j | X_{-ij}) = p(X_i | X_{-ij}) p(X_j | X_{-ij}) \quad \Rightarrow \quad X_i \ci X_j | X_{-ij}.$$

\paragraph{Question \ref{q:uncorrelated}}
As $Y = X^2$ this means $Y$ is completely determined by $X$ hence $X$ and $Y$ are dependent. However, $\mathbb{E}[XY] = \mathbb{E}[X^3] = 0$ as the PDF of a univariate Gaussian is symmetric around the mean (which is 0 in this case). Notice that $\mathbb{E}[X] = 0$, therefore $\mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[Y]$, meaning that $X$ and $Y$ are uncorrelated.

\paragraph{\questionref{q:mml-6.11}}
Let us consider the random variables $X,Y$ with joint distribution $p(x,y)$.
\begin{align*}
\mathbb{E}_X[X] &= \mathbb{E}_Y\Big[\mathbb{E}_X[X|Y]\Big]\\
&= \int_{y\in\mathbb{R}} \mathbb{E}_X[X|Y] p(y)dy\\
&= \int_{y\in\mathbb{R}} \int_{x\in\mathbb{R}} x p(x|y)p(y) dxdy\\
&=  \int_{x\in\mathbb{R}} x \left(\int_{y\in\mathbb{R}} p(x,y) dy\right)dx\\
&=  \int_{x\in\mathbb{R}} x p(x) dx = \mathbb{E}_X[X]\\
\end{align*}
Notice that we just swapped the integrals and marginalized $y$ over the joint distribution to obtain $p(x)$.


\paragraph{\questionref{q:mml-6.13}}
Given a continuous random variable $X$, with \textit{cdf} $F_X(x)$, show that the random variable $Y := F_X(X)$ is uniformly distributed.
\begin{align*}
    F_Y(y) &= P(Y \leq y)\\
    &= P(F_X(X) \leq y)\\
    &= P(X \leq F_X^{-1}(y))\\
    &= F_X(F_X^{-1}(y))\\
    &= y\\
\end{align*}
Thus, we have that $Y\sim Uniform(0,1)$ because $F_Y(y)$ is the \textit{cdf} of a uniform distribution.



%\paragraph{\questionref{q:mml-6.12}} Consider the random variable $\textbf{x} \sim \mathcal{N}(\textbf{x}|\bm{\mu}_x, \bm{\Sigma}_x)$ and the following linear model
%\[
%\textbf{y} = \textbf{A}\textbf{x} + \textbf{b} + \textbf{w}
%\]
%where $\textbf{w} \sim \mathcal{N}(\textbf{w}|\textbf{0}, \textbf{Q})$.
%\begin{enumerate}
%\item Compute the likelihood $p(\textbf{y}|\textbf{x})$.
%To calculate the likelihood, we need to recall that the addition of Gaussian distributions is also a Gaussian distribution. Thus, it is sufficient for us to compute the expectation and covariance of $\textbf{y}$ given $\textbf{x}$.
%\[
%\mathbb{E}[\textbf{y}|\textbf{x}] = \mathbb{E}[\textbf{A}\textbf{x} +\textbf{b} + \textbf{w}| \textbf{x}] = \textbf{A}\textbf{x} + \textbf{b} + \textbf{0} = \textbf{A}\textbf{x} + \textbf{b}
%\]
%we note that $\mathbb{E}[\textbf{w}|\textbf{x}] = \mathbb{E}[\textbf{w}] = \textbf{0}$ because of the independence of \textbf{x} and \textbf{w}.
%\begin{align*}
%cov[\textbf{y}|\textbf{x}] &= \mathbb{E}\Big[\big(\textbf{y} - \mathbb{E}[\textbf{y}|\textbf{x}]\big)\big(\textbf{y} - \mathbb{E}[\textbf{y}|\textbf{x}]\big)^T\Big] = \mathbb{E}\Big[\big(\textbf{A}\textbf{x} + \textbf{b} + \textbf{w} - \textbf{A}\textbf{x} - \textbf{b}\big)\big(\textbf{A}\textbf{x} + \textbf{b} + \textbf{w} - \textbf{A}\textbf{x} - \textbf{b}\big)^T\Big]\\
%&=\mathbb{E}\Big[\textbf{w}\textbf{w}^T\Big] = \textbf{Q}
%\end{align*}
%$p(\textbf{y}|\textbf{x})$ is a Gaussian distribution, $\textbf{y}|\textbf{x} \sim \mathcal{N}(\textbf{y}| \textbf{A}\textbf{x} + \textbf{b}, \textbf{Q})$.
%\item Compute the marginal distribution $p(\textbf{y})$.
%
%We need to marginalise $\textbf{x}$ from the joint distribution $p(\textbf{x},\textbf{y})$.
%\begin{equation}
%p(\textbf{y}) = \int_{\textbf{x}\in\mathbb{R}^E} p(\textbf{y},\textbf{x})d\textbf{x} = \int_{\textbf{x}\in\mathbb{R}^E} p(\textbf{y}|\textbf{x}) p(\textbf{x})d\textbf{x} = \int_{\textbf{x}\in\mathbb{R}^E} \mathcal{N}(\textbf{y}| \textbf{A}\textbf{x} + \textbf{b}, \textbf{Q}) \mathcal{N}(\textbf{x}| \bm{\mu}_x, \bm{\Sigma}_x)d\textbf{x}
%\end{equation}
%The usual procedure for computing marginals is to first marginalise w.r.t $\textbf{x}$, and then complete the square w.r.t $\textbf{y}$. We start from the following idea: marginalising a variable from a Gaussian distribution results in another Gaussian distribution. Let us denote the exponential terms of the product $p(\textbf{y}|\textbf{x}) p(\textbf{x})$.
%\begin{align*}
%    -\frac{1}{2}(\textbf{x} - \bm{\mu}_x)^T\bm{\Sigma}_x^{-1}(\textbf{x} - \bm{\mu}_x) -\frac{1}{2}(\textbf{y} - \textbf{A}\textbf{x} - \textbf{b})^T\textbf{Q}^{-1}(\textbf{y} - \textbf{A}\textbf{x} - \textbf{b})
%\end{align*}
%We now expand these expression and keep the quadratic and linear terms with respect to $\textbf{y}$ and $\textbf{x}$.
%\begin{align*}
%&=-\frac{1}{2}\textbf{x}^T\bm{\Sigma}_x^{-1}\textbf{x} 
%-\frac{1}{2}\textbf{x}^T\textbf{A}^T\textbf{Q}^{-1}\textbf{A}\textbf{x}
%-\frac{1}{2}\textbf{y}^T\textbf{Q}^{-1}\textbf{y}
%+ \textbf{x}^T\textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b}) + \textbf{x}^T\bm{\Sigma}_x^{-1}\bm{\mu}_x + \textbf{y}^T\textbf{Q}^{-1}\textbf{b} + const\\
%&=-\frac{1}{2}\textbf{x}^T\Big(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A}\Big)\textbf{x}  + \textbf{x}^T\Big(\bm{\Sigma}_x^{-1}\bm{\mu}_x + \textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b})\Big) -\frac{1}{2}\textbf{y}^T\textbf{Q}^{-1}\textbf{y} + \textbf{y}^T\textbf{Q}^{-1}\textbf{b} + const
%\end{align*}
%For simplicity, let us denote $\bm{\Lambda} = (\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A}\Big)$ and $\textbf{m} = \Big(\bm{\Sigma}_x^{-1}\bm{\mu}_x + \textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b})\Big)$. Consider now the terms that are dependent on $\textbf{x}$ using these definitions.
%\begin{equation}
%-\frac{1}{2}\textbf{x}^T\bm{\Lambda}\textbf{x}  + \textbf{x}^T\textbf{m} = -\frac{1}{2}(\textbf{x} - \bm{\Lambda}^{-1}\textbf{m})^T\bm{\Lambda}(\textbf{x} - \bm{\Lambda}^{-1}\textbf{m})  + \frac{1}{2}\textbf{m}^T\bm{\Lambda}^{-1}\textbf{m}
%\end{equation}
%Notice that if we integrate $\textbf{x}$, the first term will vanish as it is part of the integral of a normalised distribution (which is equal to 1), and translates as a 0 in the exponential terms. The second term $\frac{1}{2}\textbf{m}^T\bm{\Lambda}^{-1}\textbf{m}$ will remain unchanged. The final step is to complete the square with respect to $\textbf{y}$.
%\begin{align*}
%&=\frac{1}{2}\textbf{m}^T\bm{\Lambda}^{-1}\textbf{m} - \frac{1}{2}\textbf{y}^T\textbf{Q}^{-1}\textbf{y} + \textbf{y}^T\textbf{Q}^{-1}\textbf{b} + const\\
%&=\frac{1}{2}\Big(\bm{\Sigma}_x^{-1}\bm{\mu}_x + \textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b})\Big)^T\bm{\Lambda}^{-1}\Big(\bm{\Sigma}_x^{-1}\bm{\mu}_x + \textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b}) \Big) - \frac{1}{2}\textbf{y}^T\textbf{Q}^{-1}\textbf{y} + \textbf{y}^T\textbf{Q}^{-1}\textbf{b} + const\\
%&=-\frac{1}{2}\textbf{y}^T\Big(\textbf{Q}^{-1} - \textbf{Q}^{-1}\textbf{A}\bm{\Lambda}^{-1}\textbf{A}^T\textbf{Q}^{-1}\Big)\textbf{y} + \textbf{y}^t\textbf{Q}^{-1}\Big(\textbf{A}\bm{\Lambda}^{-1}(\bm{\Sigma}_x^{-1}\bm{\mu}_x - A^TQ^{-1}\textbf{b} )+ \textbf{b} \Big) + const
%\end{align*}
%
%We can extract both the covariance and mean from the quadratic and linear terms respectively. The covariance matrix is the following
%\[
%\bm{\Gamma} = \Big(\textbf{Q}^{-1} - \textbf{Q}^{-1}\textbf{A}\bm{\Lambda}^{-1}\textbf{A}^T\textbf{Q}^{-1}\Big)^{-1}
%\]
%We can simplify the previous expression by making use of the Woodbury matrix identity (you can google search it if you are curious).
%\[
%(\textbf{A} - \textbf{U}\textbf{C}\textbf{V})^{-1} = \textbf{A}^{-1} + \textbf{A}^{-1}\textbf{U}(\textbf{C}^{-1} - \textbf{V}\textbf{A}^{-1}\textbf{U})^{-1}\textbf{V}\textbf{A}^{-1}
%\]
%with $\textbf{A} = \textbf{Q}^{-1}$, $\textbf{U} = \textbf{Q}^{-1}\textbf{A}$, $\textbf{C} = \bm{\Lambda}^{-1}$, and $\textbf{V} = \textbf{A}^T \textbf{Q}^{-1}$ we can simplify the expression of the covariance matrix.
%
%\begin{align*}
%    \bm{\Gamma} &= \Big(\textbf{Q}^{-1} - \textbf{Q}^{-1}\textbf{A}\bm{\Lambda}^{-1}\textbf{A}^T\textbf{Q}^{-1}\Big) \\
%    &= \textbf{Q} +  \textbf{Q}\textbf{Q}^{-1}\textbf{A}\Big(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A} - \textbf{A}^T\textbf{Q}^{-1}\textbf{Q}\textbf{Q}^{-1}\textbf{A}\Big)^{-1}\textbf{A}^T\\
%    &= \textbf{Q} + \textbf{A}\bm{\Sigma}_x\textbf{A}^T
%\end{align*}
%The mean is computed using the previous result and the linear terms w.r.t. $\textbf{y}$.
%\begin{align*}
%    \bm{\mu}_{y} &= \Big(\textbf{Q} + \textbf{A}\bm{\Sigma}_x\textbf{A}^T\Big)\textbf{Q}^{-1}\Big(\textbf{A}\bm{\Lambda}^{-1}(\bm{\Sigma}_x^{-1}\bm{\mu}_x - \textbf{A}^T\textbf{Q}^{-1}\textbf{b} )+ \textbf{b} \Big)\\
%    &= \Big(\textbf{I} + \textbf{A}\bm{\Sigma}_x\textbf{A}^T\textbf{Q}^{-1}\Big)\textbf{A}(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A})^{-1}(\bm{\Sigma}_x^{-1}\bm{\mu}_x - \textbf{A}^T\textbf{Q}^{-1}\textbf{b}) + \textbf{b} + \textbf{A}\bm{\Sigma}_x \textbf{A}^T \textbf{Q}^{-1}\textbf{b}\\
%    &= \textbf{A}\Big(\textbf{I} + \bm{\Sigma}_x\textbf{A}^T\textbf{Q}^{-1}\textbf{A}\Big)\Big(\bm{\Sigma}_x^{-1}(\textbf{I} + \bm{\Sigma}_x\textbf{A}^T\textbf{Q}^{-1}\textbf{A})\Big)^{-1}(\bm{\Sigma}_x^{-1}\bm{\mu}_x - \textbf{A}^T\textbf{Q}^{-1}\textbf{b})+ \textbf{b} + \textbf{A}\bm{\Sigma}_x \textbf{A}^T \textbf{Q}^{-1}\textbf{b}\\
%    &= \textbf{A}\Big(\textbf{I} + \bm{\Sigma}_x\textbf{A}^T\textbf{Q}^{-1}\textbf{A}\Big)\Big(\textbf{I} + \bm{\Sigma}_x\textbf{A}^T\textbf{Q}^{-1}\textbf{A}\Big)^{-1}\bm{\Sigma}_x(\bm{\Sigma}_x^{-1}\bm{\mu}_x - \textbf{A}^T\textbf{Q}^{-1}\textbf{b})+ \textbf{b} + \textbf{A}\bm{\Sigma}_x \textbf{A}^T \textbf{Q}^{-1}\textbf{b}\\
%    &= \textbf{A}\bm{\mu}_x - \textbf{A}\bm{\Sigma}_x\textbf{A}^T\textbf{Q}^{-1}\textbf{b} + \textbf{b} + \textbf{A}\bm{\Sigma}_x\textbf{A}^T\textbf{Q}^{-1}\textbf{b}\\
%    &= \textbf{A}\bm{\mu}_x + \textbf{b}
%\end{align*}
%
%Finally, we get to the following marginal distribution
%\begin{equation}
%p(\textbf{y)} = \mathcal{N}(\textbf{y} | \textbf{A}\bm{\mu}_x + \textbf{b},  \textbf{Q} + \textbf{A}\bm{\Sigma}_x\textbf{A}^T).
%\end{equation}
%
%\item Compute the posterior distribution $p(\textbf{x}|\hat{\textbf{y}})$.
%
%Although it might seem complicated to compute, the posterior distribution for this case is fairly simple. We start by noticing the usual relationship derived from the Bayes' rule.
%\[
%p(\textbf{x}|\textbf{y}) = \frac{p(\textbf{x})p(\textbf{y}|\textbf{x})}{p(\textbf{y})} \propto p(\textbf{x})p(\textbf{y}|\textbf{x})
%\]
%As we can see, the joint distribution is the unnormalized distribution of the posterior. Just by completing the square with respect to $\textbf{x}$, one can find the normalized distribution. Let us denote the exponential term in the joint distribution.
%\[
%-\frac{1}{2}(\textbf{x} - \bm{\mu}_x)^T\bm{\Sigma}_x^{-1}(\textbf{x} - \bm{\mu}_x) -\frac{1}{2}(\textbf{y} - \textbf{A}\textbf{x} - \textbf{b})^T\textbf{Q}^{-1}(\textbf{y} - \textbf{A}\textbf{x} - \textbf{b})
%\]
%We will take the quadratic and linear terms with respect to \textbf{x}.
%\begin{align*}
%&-\frac{1}{2}\textbf{x}^T\bm{\Sigma}_x^{-1}\textbf{x} -\frac{1}{2}\textbf{x}^T\textbf{A}^T\textbf{Q}^{-1}\textbf{A}\textbf{x}
%+ \textbf{x}^T\bm{\Sigma}_x^{-1}\bm{\mu}_x + \textbf{x}^T\textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b}) + const\\
%&-\frac{1}{2}\textbf{x}^T\Big(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A} \Big)\textbf{x} + \textbf{x}^T\Big( \textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b}) + \bm{\Sigma}_x^{-1}\bm{\mu}_x \Big) + const
%\end{align*}
%When completing the square, we can easily extract the mean and covariance terms from the linear and quadratic forms of the exponential respectively. Thus
%\[
%cov[\textbf{x}|\textbf{y}] = \Big(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A} \Big)^{-1}
%\]
%\[
%\mathbb{E}[\textbf{x}|\textbf{y}] = cov[\textbf{x}|\textbf{y}] \Big( \textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b}) + \bm{\Sigma}_x^{-1}\bm{\mu}_x \Big) =  \Big(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A} \Big)^{-1}\Big( \textbf{A}^T\textbf{Q}^{-1}(\textbf{y} - \textbf{b}) + \bm{\Sigma}_x^{-1}\bm{\mu}_x \Big)
%\]
%As we can see, the posterior distribution for a meassurement $\hat{\textbf{y}}$ is a Gaussian expressed as
%\[
%p(\textbf{x}|\hat{\textbf{y}}) = \mathcal{N}\bigg(\textbf{x}| \Big(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A} \Big)^{-1}\Big( \textbf{A}^T\textbf{Q}^{-1}(\hat{\textbf{y}} - \textbf{b}) + \bm{\Sigma}_x^{-1}\bm{\mu}_x \Big), \Big(\bm{\Sigma}_x^{-1} + \textbf{A}^T\textbf{Q}^{-1}\textbf{A} \Big)^{-1}\bigg)
%\]
%\end{enumerate}